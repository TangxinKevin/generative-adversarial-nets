{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import minmax_scale, LabelBinarizer\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ds = datasets.load_iris()\n",
    "X_train, y_train = minmax_scale(ds.data), LabelBinarizer().fit_transform(ds.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "n_X_samples = X_train.shape[0]\n",
    "n_X_features = X_train.shape[1]\n",
    "n_Z_features = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=[None, n_X_features], name='X')\n",
    "D_W1 = tf.Variable(tf.random_uniform([n_X_features, 6]), name='D_W1')\n",
    "D_b1 = tf.Variable(tf.random_uniform([6]), name='D_b1')\n",
    "D_W2 = tf.Variable(tf.random_uniform([6, 1]), name='D_W2')\n",
    "D_b2 = tf.Variable(tf.random_uniform([1]), name='D_b2')\n",
    "D_parameters = [D_W1, D_W2, D_b1, D_b2]\n",
    "def D_logit(X):\n",
    "    D_h1 = tf.nn.tanh(tf.matmul(X, D_W1) + D_b1)\n",
    "    return tf.matmul(D_h1, D_W2) + D_b2\n",
    "\n",
    "Z = tf.placeholder(tf.float32, shape=[None, n_Z_features], name='Z')\n",
    "G_W1 = tf.Variable(tf.random_uniform([n_Z_features, 6]), name='G_W1')\n",
    "G_b1 = tf.Variable(tf.random_uniform([6]), name='G_b1')\n",
    "G_W2 = tf.Variable(tf.random_uniform([6, n_X_features]), name='G_W2')\n",
    "G_b2 = tf.Variable(tf.random_uniform([n_X_features]), name='G_b2')\n",
    "G_parameters = [G_W1, G_W2, G_b1, G_b2]\n",
    "def G_logit(Z):\n",
    "    G_h1 = tf.nn.tanh(tf.matmul(Z, G_W1) + G_b1)\n",
    "    return tf.matmul(G_h1, G_W2) + G_b2\n",
    "\n",
    "def sample_Z(n_samples, n_features):\n",
    "    return np.random.uniform(-1., 1., size=[n_samples, n_features]).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "D_logit_data = D_logit(X)\n",
    "D_logit_generated = D_logit(tf.nn.sigmoid(G_logit(Z)))\n",
    "\n",
    "D_loss_data = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_data, labels=tf.ones_like(D_logit_data)))\n",
    "D_loss_generated = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_generated, labels=tf.zeros_like(D_logit_generated)))\n",
    "D_loss = D_loss_data + D_loss_generated\n",
    "\n",
    "G_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=D_logit_generated, labels=tf.ones_like(D_logit_generated)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "D_solver = tf.train.AdamOptimizer().minimize(D_loss, var_list=D_parameters)\n",
    "G_solver = tf.train.AdamOptimizer().minimize(G_loss, var_list=G_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, discriminator loss: 3.394270181655884, generator loss: 0.03680562227964401\n",
      "Epoch: 1, discriminator loss: 3.3111073970794678, generator loss: 0.0431181862950325\n",
      "Epoch: 2, discriminator loss: 3.1722819805145264, generator loss: 0.04437081143260002\n",
      "Epoch: 3, discriminator loss: 3.118732213973999, generator loss: 0.050017885863780975\n",
      "Epoch: 4, discriminator loss: 2.9895150661468506, generator loss: 0.05530524253845215\n",
      "Epoch: 5, discriminator loss: 2.8761837482452393, generator loss: 0.06776635348796844\n",
      "Epoch: 6, discriminator loss: 2.7620742321014404, generator loss: 0.06646376848220825\n",
      "Epoch: 7, discriminator loss: 2.755126714706421, generator loss: 0.07918612658977509\n",
      "Epoch: 8, discriminator loss: 2.616192102432251, generator loss: 0.08883442729711533\n",
      "Epoch: 9, discriminator loss: 2.551145315170288, generator loss: 0.09537789970636368\n",
      "Epoch: 10, discriminator loss: 2.4858713150024414, generator loss: 0.10494150966405869\n",
      "Epoch: 11, discriminator loss: 2.398826837539673, generator loss: 0.11200179159641266\n",
      "Epoch: 12, discriminator loss: 2.3177990913391113, generator loss: 0.1281793862581253\n",
      "Epoch: 13, discriminator loss: 2.2319881916046143, generator loss: 0.1470322161912918\n",
      "Epoch: 14, discriminator loss: 2.163299083709717, generator loss: 0.158208966255188\n",
      "Epoch: 15, discriminator loss: 2.0979807376861572, generator loss: 0.17397315800189972\n",
      "Epoch: 16, discriminator loss: 1.9937264919281006, generator loss: 0.19910195469856262\n",
      "Epoch: 17, discriminator loss: 1.9271571636199951, generator loss: 0.22908714413642883\n",
      "Epoch: 18, discriminator loss: 1.7999125719070435, generator loss: 0.26218563318252563\n",
      "Epoch: 19, discriminator loss: 1.747607946395874, generator loss: 0.3019559979438782\n",
      "Epoch: 20, discriminator loss: 1.6518374681472778, generator loss: 0.34898096323013306\n",
      "Epoch: 21, discriminator loss: 1.5919805765151978, generator loss: 0.403872013092041\n",
      "Epoch: 22, discriminator loss: 1.5057562589645386, generator loss: 0.4651242196559906\n",
      "Epoch: 23, discriminator loss: 1.4659215211868286, generator loss: 0.5242273211479187\n",
      "Epoch: 24, discriminator loss: 1.4129352569580078, generator loss: 0.5850614309310913\n",
      "Epoch: 25, discriminator loss: 1.3880677223205566, generator loss: 0.636492133140564\n",
      "Epoch: 26, discriminator loss: 1.3653714656829834, generator loss: 0.678872287273407\n",
      "Epoch: 27, discriminator loss: 1.3534023761749268, generator loss: 0.7131833434104919\n",
      "Epoch: 28, discriminator loss: 1.3377320766448975, generator loss: 0.7330964803695679\n",
      "Epoch: 29, discriminator loss: 1.3338298797607422, generator loss: 0.7592644691467285\n",
      "Epoch: 30, discriminator loss: 1.3282628059387207, generator loss: 0.7610554099082947\n",
      "Epoch: 31, discriminator loss: 1.3701708316802979, generator loss: 0.7798106074333191\n",
      "Epoch: 32, discriminator loss: 1.330758810043335, generator loss: 0.7780892252922058\n",
      "Epoch: 33, discriminator loss: 1.3681137561798096, generator loss: 0.7875648736953735\n",
      "Epoch: 34, discriminator loss: 1.3445379734039307, generator loss: 0.7786401510238647\n",
      "Epoch: 35, discriminator loss: 1.359877347946167, generator loss: 0.7805898189544678\n",
      "Epoch: 36, discriminator loss: 1.3831192255020142, generator loss: 0.7676979899406433\n",
      "Epoch: 37, discriminator loss: 1.3256025314331055, generator loss: 0.7623332142829895\n",
      "Epoch: 38, discriminator loss: 1.3808999061584473, generator loss: 0.7657303810119629\n",
      "Epoch: 39, discriminator loss: 1.329134225845337, generator loss: 0.7576708793640137\n",
      "Epoch: 40, discriminator loss: 1.3812459707260132, generator loss: 0.7543315291404724\n",
      "Epoch: 41, discriminator loss: 1.3625998497009277, generator loss: 0.7628188133239746\n",
      "Epoch: 42, discriminator loss: 1.365304946899414, generator loss: 0.7504124641418457\n",
      "Epoch: 43, discriminator loss: 1.3423385620117188, generator loss: 0.7457904815673828\n",
      "Epoch: 44, discriminator loss: 1.3351505994796753, generator loss: 0.7393727898597717\n",
      "Epoch: 45, discriminator loss: 1.3255577087402344, generator loss: 0.7306934595108032\n",
      "Epoch: 46, discriminator loss: 1.370158076286316, generator loss: 0.7281281352043152\n",
      "Epoch: 47, discriminator loss: 1.3514988422393799, generator loss: 0.6950126886367798\n",
      "Epoch: 48, discriminator loss: 1.377165675163269, generator loss: 0.7050179243087769\n",
      "Epoch: 49, discriminator loss: 1.3863728046417236, generator loss: 0.6929742693901062\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "batch_size = 10\n",
    "n_batches = int(n_X_samples / batch_size)\n",
    "for epoch in range(50):\n",
    "    X_epoch = X_train[np.random.permutation(range(n_X_samples))]\n",
    "    for batch_index in range(n_batches):\n",
    "        start_index = batch_index * batch_size\n",
    "        end_index = start_index + batch_size\n",
    "        X_batch = X_epoch[start_index:end_index]\n",
    "        _, D_loss_value = sess.run([D_solver, D_loss], feed_dict={X: X_batch, Z: sample_Z(batch_size, n_Z_features)})\n",
    "        _, G_loss_value = sess.run([G_solver, G_loss], feed_dict={Z: sample_Z(batch_size, n_Z_features)})\n",
    "    print('Epoch: {}, discriminator loss: {}, generator loss: {}'.format(epoch, D_loss_value, G_loss_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_generated = tf.nn.sigmoid(G_logit(sample_Z(10, n_Z_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.nn.tanh(tf.matmul(Z, G_W1) + G_b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Sigmoid_1:0' shape=(10, 4) dtype=float32>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.sigmoid(G_logit(sample_Z(10, n_Z_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.22884381, -0.01428368, -0.01331964,  0.60230684,  0.50267297,\n",
       "         0.1466855 ],\n",
       "       [ 0.41315621,  0.05289599,  0.48941782,  0.62571698,  0.41351119,\n",
       "         0.54372603],\n",
       "       [ 0.31254059,  0.1366342 ,  0.1802099 , -0.08998414, -0.03047688,\n",
       "         0.20382974],\n",
       "       [ 0.3270953 ,  0.07598057,  0.00336073,  0.00547926,  1.09153223,\n",
       "         0.12950367],\n",
       "       [ 0.30255401,  0.08746911, -0.01836257, -0.1077238 ,  0.80138707,\n",
       "         0.47222441],\n",
       "       [ 0.04228994,  0.19683203,  0.39572161,  0.51092356,  0.13156611,\n",
       "         0.08996762]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(G_W1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
